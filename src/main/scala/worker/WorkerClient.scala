package worker

import rpc.sort._
import scala.concurrent.{Future, Await, ExecutionContext}
import scala.concurrent.duration._
import io.grpc.ManagedChannelBuilder
import io.grpc.stub.StreamObserver
import com.google.protobuf.ByteString
import common.RecordIO

/** Worker ì‹¤í–‰ ì´ˆê¸° ì„¤ì • */
final case class WorkerConfig(
    masterAddr: String,
    inputPaths: Seq[String],
    outputDir: String,
)

/** Worker ì‹¤í–‰ ë©”ì¸ */
object WorkerClient {

  // ===== Fault Injector =====
  object FaultInjector {
    private val enabledPhases = sys.env.get("FAULT_INJECT_PHASE")
      .map(_.split(",").toSet)
      .getOrElse(Set.empty)
    
    private val targetWorkerId = sys.env.get("FAULT_INJECT_WORKER")
      .map(_.toInt)
      .getOrElse(-1)
    
    def checkAndCrash(phase: String): Unit = {
      if (enabledPhases.contains(phase)) {
        val myId = WorkerState.getWorkerId
        
        if (targetWorkerId == -1 || targetWorkerId == myId) {
          Console.err.println(s"\nðŸ’¥ðŸ’¥ðŸ’¥ [FAULT INJECTION] ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥")
          Console.err.println(s"ðŸ’¥ Worker $myId crashing at phase: $phase ðŸ’¥")
          Console.err.println(s"ðŸ’¥ Terminating in 2 seconds...                 ðŸ’¥")
          Console.err.println(s"ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥\n")
          Thread.sleep(2000)
          System.exit(137)
        }
      }
    }
    
    def maybeSleep(phase: String, seconds: Int = 10): Unit = {
      if (enabledPhases.contains(phase)) {
        val myId = WorkerState.getWorkerId
        
        if (targetWorkerId == -1 || targetWorkerId == myId) {
          println(s"â¸ï¸  [FAULT INJECTION] Worker $myId sleeping ${seconds}s at phase: $phase")
          Thread.sleep(seconds * 1000)
        }
      }
    }
  }

  // ===== Heartbeat Manager =====
  object HeartbeatManager {
    private var thread: Thread = _
    
    def start(workerInfo: WorkerInfo, masterClient: MasterClient): Unit = {
      thread = new Thread {
        setDaemon(true)
        override def run(): Unit = {
          while (!Thread.currentThread().isInterrupted) {
            try {
              masterClient.sendHeartbeat(workerInfo)
              Thread.sleep(2000)
            } catch {
              case _: InterruptedException => return
              case e: Exception => 
                println(s"âš ï¸ Heartbeat error: ${e.getMessage}")
            }
          }
        }
      }
      thread.start()
      println("ðŸ’“ Heartbeat started")
    }
    
    def stop(): Unit = {
      if (thread != null && thread.isAlive) {
        thread.interrupt()
        thread.join(1000)
        println("ðŸ’“ Heartbeat stopped")
      }
    }
  }

  /**
  * ë³‘ë ¬ ì •ë ¬: ë°ì´í„°ë¥¼ numThreadsê°œë¡œ ë‚˜ëˆ ì„œ ë³‘ë ¬ ì •ë ¬ í›„ K-way merge
  */
  private def parallelSort(
    records: Vector[Array[Byte]], 
    numThreads: Int = 4
  )(implicit ec: ExecutionContext): Vector[Array[Byte]] = {
    
    if (records.isEmpty) return Vector.empty
    
    println(s"ðŸ”§ Parallel sorting ${records.size} records with $numThreads threads...")
    
    val chunkSize = (records.size + numThreads - 1) / numThreads
    val chunks = records.grouped(chunkSize).toVector
    
    val sortedChunksFutures = chunks.map { chunk =>
      Future {
        chunk.sortWith { (a, b) =>
          RecordIO.compareKeys(extractKey(a), extractKey(b)) < 0
        }
      }
    }
    
    val sortedChunks = Await.result(Future.sequence(sortedChunksFutures), Duration.Inf)
    kWayMerge(sortedChunks.toList)
  }

  /**
  * K-way merge for sorted chunks
  */
  private def kWayMerge(chunks: List[Vector[Array[Byte]]]): Vector[Array[Byte]] = {
    case class ChunkIter(var current: Array[Byte], it: Iterator[Array[Byte]], chunkId: Int)
    
    implicit val chunkOrdering: Ordering[ChunkIter] =
      Ordering.fromLessThan[ChunkIter] { (x, y) =>
        RecordIO.compareKeys(extractKey(x.current), extractKey(y.current)) > 0
      }
    
    val pq = scala.collection.mutable.PriorityQueue.empty[ChunkIter]
    
    chunks.zipWithIndex.foreach { case (chunk, idx) =>
      val it = chunk.iterator
      if (it.hasNext) {
        pq.enqueue(ChunkIter(it.next(), it, idx))
      }
    }
    
    val result = scala.collection.mutable.ArrayBuffer.empty[Array[Byte]]
    
    while (pq.nonEmpty) {
      val smallest = pq.dequeue()
      result += smallest.current
      
      if (smallest.it.hasNext) {
        smallest.current = smallest.it.next()
        pq.enqueue(smallest)
      }
    }
    
    result.toVector
  }

  /**
  * Extract key from 100-byte record
  */
  private def extractKey(rec: Array[Byte]): Array[Byte] =
    java.util.Arrays.copyOfRange(rec, 0, RecordIO.KeySize)

  private def readAll(path: String): Vector[Array[Byte]] = {
    val file = new java.io.File(path)
    val files = if (file.isDirectory) {
      file.listFiles().filter(_.isFile).toSeq
    } else {
      Seq(file)
    }
    
    files.flatMap { f =>
      val buf = scala.collection.mutable.ArrayBuffer.empty[Array[Byte]]
      RecordIO.streamRecords(f.getPath) { (key, value) =>
        val rec = new Array[Byte](RecordIO.RecordSize)
        System.arraycopy(key, 0, rec, 0, RecordIO.KeySize)
        System.arraycopy(value, 0, rec, RecordIO.KeySize, RecordIO.RecordSize - RecordIO.KeySize)
        buf += rec
      }
      buf
    }.toVector
  }

  private def hasSentCheckpoints(outputDir: String): Boolean = {
    val checkpointDir = new java.io.File(s"$outputDir/sent-checkpoint")
    checkpointDir.exists() && 
      checkpointDir.listFiles().exists(_.getName.startsWith("sent_p"))
  }

  // ===== Main Entry Point =====
  def main(args: Array[String]): Unit = {

    implicit val ec: ExecutionContext = ExecutionContext.global

    sys.addShutdownHook {
      println("ðŸ›‘ Shutting down worker...")
      HeartbeatManager.stop()
    }
    
    try {
      val conf = parseArgs(args) match {
        case Some(c) => c
        case None =>
          System.exit(1)
          return
      }
      
      val masterAddr = conf.masterAddr.split(":")
      val workerInfo = WorkerInfo(
        id = -1,
        ip = getLocalIP(),
        port = 6000,
        inputDirs = conf.inputPaths,
        outputDir = conf.outputDir
      )
      
      val masterClient = new MasterClient(masterAddr(0), masterAddr(1).toInt)

      val assignment = masterClient.register(workerInfo)

      println("=" * 60)
      println(s"   âœ… Worker ${assignment.workerId} started")
      println(s"      master: ${conf.masterAddr}")
      println(s"      port: ${assignment.assignedPort}")
      println("=" * 60)
      
      val updatedWorkerInfo = workerInfo.copy(
        id = assignment.workerId,
        port = assignment.assignedPort
      )
      WorkerState.setWorkerInfo(updatedWorkerInfo)
      WorkerState.setMasterClient(masterClient)
      
      val workerServer = new WorkerServer(assignment.assignedPort, conf.outputDir)
      workerServer.start()

      HeartbeatManager.start(updatedWorkerInfo, masterClient)

      if (hasSentCheckpoints(conf.outputDir)) {
        Thread.sleep(2000)
        println("ðŸ”„ Recovery mode: waiting for finalize...")
        
        WorkerState.awaitFinalizeComplete()
        
        println("âœ… Worker work completed")
        println("â³ Waiting for shutdown...")
        WorkerState.awaitShutdownCommand()
        
        HeartbeatManager.stop()
        masterClient.shutdown()
        println("ðŸ’€ Worker shutting down...")
        return
      }

      // Sampling
      val samples = common.Sampling.uniformEveryN(conf.inputPaths, everyN = 1000)
      println(s"ðŸ“Š Collected ${samples.size} samples")

      FaultInjector.checkAndCrash("after-sampling")

      masterClient.sendSamples(samples)
      println(s"âœ… Sampling complete")

      // Load and Sort
      val allRecords: Vector[Array[Byte]] =
        conf.inputPaths.flatMap(path => readAll(path)).toVector

      println(s"ðŸ“¦ Loaded ${allRecords.size} records")

      val sorted = parallelSort(allRecords, numThreads = 4)
      println("âœ… Local sorting completed")

      FaultInjector.checkAndCrash("after-sort")

      // Partitioning
      val splitterKeys: Array[Array[Byte]] = WorkerState.getSplitters
      val partitioned = sorted.groupBy(rec => WorkerState.findPartitionId(extractKey(rec)))

      println(s"ðŸ§© Partitioned into ${partitioned.size} partitions")

      FaultInjector.checkAndCrash("after-partition")

      // Shuffle
      val workerAddresses = WorkerState.getWorkerAddresses.getOrElse {
        throw new RuntimeException("Worker addresses not available")
      }
      
      def sendPartitionWithRetry(
        originalTarget: Int,
        partitionId: Int,
        records: Seq[Array[Byte]],
        workerAddresses: Map[Int, (String, Int)],
        maxRetries: Int = 5
      ): Boolean = {
        
        var attempt = 0
        
        while (attempt < maxRetries) {              
          try {
            val (targetIp, targetPort) = workerAddresses(originalTarget)
            
            val channel = ManagedChannelBuilder
              .forAddress(targetIp, targetPort)
              .usePlaintext()
              .build()

            try {
              val stub = WorkerServiceGrpc.stub(channel)
              val ackPromise = scala.concurrent.Promise[Unit]()
              
              val responseObserver = new StreamObserver[Ack] {
                override def onNext(v: Ack): Unit = ()
                
                override def onError(t: Throwable): Unit = {
                  ackPromise.failure(t)
                }
                
                override def onCompleted(): Unit = {
                  ackPromise.success(())
                }
              }
              
              val requestObserver = stub.pushPartition(responseObserver)
              
              var seq: Long = 0
              records.foreach { rec =>
                val chunk = PartitionChunk(
                  task = Some(TaskId("task-001")),
                  partitionId = s"p$partitionId",
                  senderId = WorkerState.getWorkerId,
                  payload = ByteString.copyFrom(rec),
                  seq = seq
                )
                seq += 1
                requestObserver.onNext(chunk)
              }
              
              requestObserver.onCompleted()
              Await.result(ackPromise.future, 30.seconds)
              channel.shutdown()
              
              return true

            } finally {
              channel.shutdown()
            }
            
          } catch {
            case e: Exception =>
              attempt += 1
              
              if (attempt < maxRetries) {
                val backoff = 3000 * attempt
                Thread.sleep(backoff)
              } else {
                Console.err.println(s"âŒ Failed to send p$partitionId after $maxRetries attempts")
                return false
              }
          }
        }

        false
      }

      println("ðŸšš Starting shuffle...")

      try {      
        val maxParallel = 4
        val batches = partitioned.toSeq.grouped(maxParallel).toSeq
        
        println(s"ðŸ“¦ Sending ${partitioned.size} partitions in ${batches.size} batches")
        
        batches.zipWithIndex.foreach { case (batch, batchIdx) =>
          if (batchIdx == batches.size / 2) {
            FaultInjector.checkAndCrash("mid-shuffle")
          }
          
          val batchFutures = batch.map { case (pid, recs) =>
            Future {
              val targetWorker = WorkerState.getPartitionTargetWorker(pid)
              checkpointSentPartition(pid, recs, conf.outputDir)
              val success = sendPartitionWithRetry(targetWorker, pid, recs, workerAddresses)
              (pid, success)
            }
          }
          
          val results = Await.result(Future.sequence(batchFutures), 120.seconds)
          val (successes, failures) = results.partition(_._2)

          if (failures.nonEmpty) {
            println(s"âš ï¸  Batch ${batchIdx + 1}: ${failures.size} failed (will recover)")
          }
        }

      } catch {
        case e: Exception =>
          Console.err.println(s"âš ï¸ Shuffle errors: ${e.getMessage}")
          e.printStackTrace()
      }

      println("âœ… Shuffle completed")

      val sendRecords = partitioned.keys.map { pid =>
        val target = WorkerState.getPartitionTargetWorker(pid)
        PartitionSendRecord(
          partitionId = pid,
          targetWorkerId = target,
          senderId = WorkerState.getWorkerId,
          success = true
        )
      }.toSeq

      val report = ShuffleCompletionReport(
        workerId = WorkerState.getWorkerId,
        sendRecords = sendRecords
      )
      WorkerState.setShuffleReport(report)

      FaultInjector.checkAndCrash("before-finalize")

      try {
        WorkerState.reportShuffleComplete()
      } catch {
        case e: Exception =>
          Console.err.println(s"âš ï¸ Failed to report shuffle: ${e.getMessage}")
      }

      WorkerState.awaitFinalizeComplete()

      println("âœ… Worker work completed")
      println("â³ Waiting for shutdown...")

      WorkerState.awaitShutdownCommand()

      HeartbeatManager.stop()

      try {
        masterClient.shutdown()
      } catch {
        case e: Exception =>
          Console.err.println(s"âš ï¸ Shutdown error: ${e.getMessage}")
      }

      println("ðŸ’€ Worker shutting down...")
        
    } catch {
      case e: Exception =>
        Console.err.println(s"âŒ Worker error: ${e.getMessage}")
        e.printStackTrace()
        HeartbeatManager.stop()

        try {
          WorkerState.getMasterClient.shutdown()
        } catch {
          case _: Exception => // Ignore
        }

        System.exit(1)
    }
  }

  /** Local IPv4 ê²€ìƒ‰ */
  private def getLocalIP(): String = {
    import java.net.{InetAddress, NetworkInterface}
    import scala.jdk.CollectionConverters._

    NetworkInterface.getNetworkInterfaces.asScala
      .flatMap(_.getInetAddresses.asScala)
      .find(addr => !addr.isLoopbackAddress && addr.getAddress.length == 4)
      .map(_.getHostAddress)
      .getOrElse("127.0.0.1")
  }

  /**
   * Sender checkpoint ì €ìž¥
   */
  private def checkpointSentPartition(
    partitionId: Int, 
    records: Seq[Array[Byte]], 
    outputDir: String
  ): Unit = {
    val checkpointDir = new java.io.File(s"$outputDir/sent-checkpoint")
    checkpointDir.mkdirs()
    
    val tempFile = new java.io.File(checkpointDir, s"sent_p${partitionId}.dat.tmp")
    val fos = new java.io.FileOutputStream(tempFile)
    try {
      records.foreach { rec => fos.write(rec) }
    } finally {
      fos.close()
    }
    
    val finalFile = new java.io.File(checkpointDir, s"sent_p${partitionId}.dat")
    if (finalFile.exists()) finalFile.delete()
    tempFile.renameTo(finalFile)
  }

  private def parseArgs(args: Array[String]): Option[WorkerConfig] = {
    if (args.isEmpty) {
      printUsage()
      return None
    }

    val masterAddr = args(0)
    val inputs     = collection.mutable.ArrayBuffer.empty[String]
    var outputDir  = "./out"

    var i = 1
    def needValue(opt: String): Boolean = {
      if (i >= args.length) {
        Console.err.println(s"Missing value for $opt")
        false
      } else true
    }

    while (i < args.length) {
      args(i) match {
        case "-I" | "--input" =>
          i += 1
          if (!needValue("-I")) return None
          inputs += args(i)

        case "-O" | "--output" =>
          i += 1
          if (!needValue("-O")) return None
          outputDir = args(i)

        case other =>
          Console.err.println(s"Unknown option: $other")
          printUsage()
          return None
      }
      i += 1
    }

    if (inputs.isEmpty) {
      Console.err.println("At least one -I <input-path> is required.")
      printUsage()
      None
    } else {
      Some(
        WorkerConfig(
          masterAddr = masterAddr,
          inputPaths = inputs.toSeq,
          outputDir  = outputDir
        )
      )
    }
  }

  private def printUsage(): Unit = {
    val msg =
      """Usage:
        |  worker <master IP:port> -I <input directory> [<input directory> ...] -O <output directory>
        |
        |Example:
        |  worker 141.223.91.80:30040 -I /data1/input /data2/input -O /home/gla/data
        |""".stripMargin
    Console.err.println(msg)
  }
}